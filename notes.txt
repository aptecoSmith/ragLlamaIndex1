install ollama
https://ollama.com/

once installed, in a terminal / cmd
ollama pull mistral
ollama pull mixtral



https://docs.llamaindex.ai/en/latest/getting_started/installation.html

pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface


https://docs.llamaindex.ai/en/latest/getting_started/starter_example_local.html

The base llama_index libraries default to openAI - you have to pip individually to avoid this.

Some classes have dependencies called mid-method.  There were two examples of this I found:
indices/base.py
and
runner/base.py

The amended methods are in text files for you to copy in.

--------The embeddings use a local postgres vector db

install pgVector:
https://github.com/pgvector/pgvector

IIRC -
Ensure Visual studio has c++

install chocolatey ( there may be another way, I dunno)
choco install make

cmd window
set "PGROOT=C:\Program Files\PostgreSQL\16"
git clone --branch v0.6.0 https://github.com/pgvector/pgvector.git
cd pgvector
nmake /F Makefile.win
nmake /F Makefile.win install


call "C:\Program Files\Microsoft Visual Studio\2022\Professional\VC\Auxiliary\Build\vcvars64.bat"

---------------------
The RAG pulls docs from a markdown folder

Put the markdowns in the markdowns folder

These markdowns will be embedded in your pgVector db
Each markdown will be hashed and added to processed_files.txt.
On each run, the markdowns are hashed again and any that have changed are re-embedded.

Delete processed_files.txt, and delete the table in pGAdmin, to re embed afresh.


-----

The file starter.py is where you should start.

Scroll down to the bottom and you'll see lots of commented out method calls.  One should not be commented.
